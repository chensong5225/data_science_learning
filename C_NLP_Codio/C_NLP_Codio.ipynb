{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.coursera.org/learn/codio-select-topics-python-natural-language-processing/\n",
    "\n",
    "# 1 NLP basic workflow\n",
    "## 1.1 Getting started: corpora, lexicon, and tokens\n",
    "\n",
    "- Corpus & Corpora\n",
    "    - Corpus is one collection of texts\n",
    "    - Corpora in NLTK: Brown, Gutenberg, Web Text, Inaugural, ...\n",
    "- Lexicon\n",
    "    - a collection of words / phrases marked with allied information\n",
    "    - lexicon is a type of corpus\n",
    "    - Lexicons in NLTK: stopwords, names, CMU pronouncing dictionary, ...\n",
    "- wordnet\n",
    "    - wordnet is an english dictionary used to defin single words and phrases\n",
    "    - wordnet is a lexical corpus in NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "# intall nltk\n",
    "python3 -m pip install nltk==3.6.7\n",
    "\n",
    "# check package install status\n",
    "python3 -m pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download nltk data\n",
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# access corpora\n",
    "from nltk.corpus import brown\n",
    "from nltk.corpus import gutenberg\n",
    "from nltk.corpus import webtext\n",
    "from nltk.corpus import nps_chat\n",
    "from nltk.corpus import reuters\n",
    "from nltk.corpus import inaugural\n",
    "\n",
    "# a list of words in corpus\n",
    "brown.words()\n",
    "\n",
    "# a list of sentences in corpus, each sentence is a list of words\n",
    "brown.sents()\n",
    "\n",
    "# a list of paragraph in corpus, each paragraph is a list of sentences\n",
    "brown.paras()\n",
    "\n",
    "# a list of tuples, each tuple is (word, tag)\n",
    "brown.tagged_words()\n",
    "\n",
    "# tagged sentences and paragraphs\n",
    "brown.tagged_sents()\n",
    "brown.tagged_paras()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lexicons\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import names\n",
    "from nltk.corpus import cmudict\n",
    "\n",
    "# English stopwords\n",
    "stopwords.words('english')\n",
    "\n",
    "# names\n",
    "male_names = names.words('male.txt')\n",
    "female_names = names.words('female.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wordnet\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "# find synonym set of a word\n",
    "hello_synset = wordnet.synsets('hello')\n",
    "print(f\"\\nThe Synonym Set for the word 'hello' is: {hello_synset}\")\n",
    "\n",
    "# find lemma names of synonym sets\n",
    "# lemmas are the names of groups of words, which have varying forms but refer to the same meaning, i.e. walk and walked\n",
    "hello_lemma_names = wordnet.synset('hello.n.01').lemma_names()\n",
    "print(f\"\\nThe Lemma Names in the Synonym Set 'hello.n.01' are: {hello_lemma_names}\")\n",
    "\n",
    "\n",
    "# find lemmas of a word\n",
    "hello_lemmas =  wordnet.lemmas('hello') \n",
    "print(f\"\\nThe Lemmas in the Word 'hello' are: {hello_lemmas}\")\n",
    "\n",
    "# find lemmas of a synonym set\n",
    "hello_n_01_synset_lemmas = wordnet.synset('hello.n.01').lemmas()\n",
    "print(f\"\\nThe Lemmas in the Synonym Set 'hello.n.01' are: {hello_n_01_synset_lemmas}\")\n",
    "\n",
    "\n",
    "# find synset and synset name of a lemma\n",
    "hello_n_01_hello_lemma_synset = wordnet.lemma('hello.n.01.hello').synset()\n",
    "print(f\"\\nThe Synset for the Lemma 'hello.n.01.hello' is: {hello_n_01_hello_lemma_synset}\")\n",
    "\n",
    "hello_n_01_hello_lemma_synset_name = wordnet.lemma('hello.n.01.hello').name()\n",
    "print(f\"\\nThe Word / Synset Name for the Lemma 'hello.n.01.hello' is: {hello_n_01_hello_lemma_synset_name}\")\n",
    "\n",
    "\n",
    "# find definitions and examples of a synonym\n",
    "hello_def = wordnet.synset('hello.n.01').definition()\n",
    "print(f\"\\nThe Definition for the Synonym Set 'hello.n.01' from the word 'hello' is: {hello_def}\")\n",
    "\n",
    "hello_examples = wordnet.synset('hello.n.01').examples()\n",
    "print(f\"\\nExamples for the Synonym Set 'hello.n.01' from the word 'hello' is: {hello_examples}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Tokenize Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize by word\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "lorenzo_paragraph = \"Lorenzo di Piero de'Medici was an Italian statesman, banker, de facto ruler of the Florentine Republic and the most powerful and enthusiastic patron of Renaissance culture in Italy. Also known as Lorenzo the Magnificent (Lorenzo il Magnifico) by contemporary Florentines, he was a magnate, diplomat, politician and patron of scholars, artists, and poets. As a patron, he's best known for his sponsorship of artists such as Botticelli and Michelangelo. He held the balance of power within the Italic League, an alliance of states that stabilized political conditions on the Italian peninsula for decades, and his life coincided with the mature phase of the Italian Renaissance and the Golden Age of Florence.\"\n",
    "word_tokenize(lorenzo_paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize by sentence\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "lorenzo_paragraph = \"Lorenzo di Piero de'Medici was an Italian statesman, banker, de facto ruler of the Florentine Republic and the most powerful and enthusiastic patron of Renaissance culture in Italy. Also known as Lorenzo the Magnificent (Lorenzo il Magnifico) by contemporary Florentines, he was a magnate, diplomat, politician and patron of scholars, artists, and poets. As a patron, he's best known for his sponsorship of artists such as Botticelli and Michelangelo. He held the balance of power within the Italic League, an alliance of states that stabilized political conditions on the Italian peninsula for decades, and his life coincided with the mature phase of the Italian Renaissance and the Golden Age of Florence.\"\n",
    "sent_tokenize(lorenzo_paragraph)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Access Text from different resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract text from the web\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "extinction_url = 'https://www.bbc.com/news/science-environment-61242789'\n",
    "extinction_html = urlopen(extinction_url)\n",
    "extinction_html_parse = BeautifulSoup(extinction_html, 'html.parser')\n",
    "\n",
    "for index, element in enumerate(extinction_html_parse.find_all('p')):\n",
    "  words = element.get_text()\n",
    "  print(f'\\nTokens in Paragraph {index + 1}: {word_tokenize(words)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract text from local files\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "with open('exercises/local_article.txt') as local_text_file:\n",
    "  raw_local_text_file = local_text_file.read()\n",
    "  print(f\"\\nTokens from the Local Article Called 'local_article.txt': {word_tokenize(raw_local_text_file)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You typed 5 words.\n"
     ]
    }
   ],
   "source": [
    "# use text from user input\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "user_input = input()\n",
    "print(f'You typed {len(word_tokenize(user_input))} words.')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Method for analyze natural language\n",
    "## 2.1 letter frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Methods from https://www.nostarch.com/crackingcodes (BSD Licensed)\n",
    "\n",
    "ETAOIN = 'ETAOINSHRDLCUMWFGYPBVKJXQZ'\n",
    "LETTERS = 'ABCDEFGHIJKLMNOPQRSTUVWXYZ'\n",
    "\n",
    "def getLetterCount(message):\n",
    "    # Returns a dictionary with keys of single letters and values of the\n",
    "    # count of how many times they appear in the message parameter:\n",
    "    letterCount = {'A': 0, 'B': 0, 'C': 0, 'D': 0, 'E': 0, 'F': 0, 'G': 0, 'H': 0, 'I': 0, 'J': 0, 'K': 0, 'L': 0, 'M': 0, 'N': 0, 'O': 0, 'P': 0, 'Q': 0, 'R': 0, 'S': 0, 'T': 0, 'U': 0, 'V': 0, 'W': 0, 'X': 0, 'Y': 0, 'Z': 0}\n",
    "\n",
    "    for letter in message.upper():\n",
    "        if letter in LETTERS:\n",
    "            letterCount[letter] += 1\n",
    "\n",
    "    return letterCount\n",
    "\n",
    "def getItemAtIndexZero(items):\n",
    "    return items[0]\n",
    "\n",
    "def getFrequencyOrder(message):\n",
    "    # Returns a string of the alphabet letters arranged in order of most\n",
    "    # frequently occurring in the message parameter.\n",
    "\n",
    "    # First, get a dictionary of each letter and its frequency count:\n",
    "    letterToFreq = getLetterCount(message)\n",
    "\n",
    "    # Second, make a dictionary of each frequency count to each letter(s)\n",
    "    # with that frequency:\n",
    "    freqToLetter = {}\n",
    "    for letter in LETTERS:\n",
    "        if letterToFreq[letter] not in freqToLetter:\n",
    "            freqToLetter[letterToFreq[letter]] = [letter]\n",
    "        else:\n",
    "            freqToLetter[letterToFreq[letter]].append(letter)\n",
    "\n",
    "    # Third, put each list of letters in reverse \"ETAOIN\" order, and then\n",
    "    # convert it to a string:\n",
    "    for freq in freqToLetter:\n",
    "        freqToLetter[freq].sort(key=ETAOIN.find, reverse=True)\n",
    "        freqToLetter[freq] = ''.join(freqToLetter[freq])\n",
    "\n",
    "    # Fourth, convert the freqToLetter dictionary to a list of\n",
    "    # tuple pairs (key, value), then sort them:\n",
    "    freqPairs = list(freqToLetter.items())\n",
    "    freqPairs.sort(key=getItemAtIndexZero, reverse=True)\n",
    "\n",
    "    # Fifth, now that the letters are ordered by frequency, extract all\n",
    "    # the letters for the final string:\n",
    "    freqOrder = []\n",
    "    for freqPair in freqPairs:\n",
    "        freqOrder.append(freqPair[1])\n",
    "\n",
    "    return ''.join(freqOrder)\n",
    "\n",
    "def main():\n",
    "    message = \"\"\"\n",
    "    Alan Mathison Turing was an English mathematician, computer scientist, logician, \n",
    "    cryptanalyst, philosopher, and theoretical biologist. Turing was highly influential \n",
    "    in the development of theoretical computer science, providing a formalisation of the \n",
    "    concepts of algorithm and computation with the Turing machine, which can be considered \n",
    "    a model of a general-purpose computer. He is widely considered to be the father of \n",
    "    theoretical computer science and artificial intelligence.\n",
    "    \n",
    "    Born in Maida Vale, London, Turing was raised in southern England. He graduated at \n",
    "    King's College, Cambridge, with a degree in mathematics. Whilst he was a fellow at \n",
    "    Cambridge, he published a proof demonstrating that some purely mathematical yes–no questions \n",
    "    can never be answered by computation and defined a Turing machine, and went on to prove \n",
    "    that the halting problem for Turing machines is undecidable. In 1938, he obtained his PhD \n",
    "    from the Department of Mathematics at Princeton University. During the Second World War, \n",
    "    Turing worked for the Government Code and Cypher School (GC&CS) at Bletchley Park, \n",
    "    Britain's codebreaking centre that produced Ultra intelligence. For a time he led Hut 8, \n",
    "    the section that was responsible for German naval cryptanalysis. Here, he devised a number \n",
    "    of techniques for speeding the breaking of German ciphers, including improvements to the \n",
    "    pre-war Polish bombe method, an electromechanical machine that could find settings for the \n",
    "    Enigma machine. Turing played a crucial role in cracking intercepted coded messages that \n",
    "    enabled the Allies to defeat the Axis powers in many crucial engagements, including the \n",
    "    Battle of the Atlantic.\n",
    "\n",
    "    After the war, Turing worked at the National Physical Laboratory, where he designed the \n",
    "    Automatic Computing Engine (ACE), one of the first designs for a stored-program computer. \n",
    "    In 1948, Turing joined Max Newman's Computing Machine Laboratory, at the Victoria \n",
    "    University of Manchester, where he helped develop the Manchester computers and became \n",
    "    interested in mathematical biology. He wrote a paper on the chemical basis of morphogenesis\n",
    "    and predicted oscillating chemical reactions such as the Belousov–Zhabotinsky reaction, \n",
    "    first observed in the 1960s. Despite these accomplishments, Turing was never fully recognised\n",
    "    in Britain during his lifetime because much of his work was covered by the Official Secrets Act.\n",
    "    \"\"\"\n",
    "    ## example from https://en.wikipedia.org/wiki/Alan_Turing\n",
    "\n",
    "    print(getLetterCount(message))\n",
    "\n",
    "# Run main function\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Word Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import webtext\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "wt_words = webtext.words()\n",
    "data_analysis = FreqDist(wt_words)\n",
    " \n",
    "# Let's take the specific words only if their frequency is greater than 3.\n",
    "filter_words = dict([(m, n) for m, n in data_analysis.items() if len(m) > 3])\n",
    "data_analysis = FreqDist(filter_words)\n",
    "\n",
    "data_analysis.plot(25, cumulative=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print top 25 frequent words\n",
    "most_common = data_analysis.most_common(25)\n",
    "print(\"\\nIn Order of Most Common Frequencies: \")\n",
    "for word in most_common:\n",
    "  print(word)\n",
    "\n",
    "# print most frequent word\n",
    "print(\"\\nMost common word: \")\n",
    "print(data_analysis.max())\n",
    "\n",
    "# print words show up more than 1000 times\n",
    "most_common = data_analysis.most_common()\n",
    "print(\"\\nIn Order of Most Common Frequencies: \")\n",
    "for word in most_common:\n",
    "    if word[1] > 1000:\n",
    "        print(word)\n",
    "\n",
    "# hapaxes are words show up exactly once\n",
    "hapaxes = data_analysis.hapaxes()\n",
    "print(\"\\nHapaxes or Words occuring only once: \")\n",
    "for word in hapaxes:\n",
    "    print(word)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Bag of words\n",
    "- similar to word cloud, a bag of words\n",
    "    - ignore the order of words\n",
    "    - represented as a set of unique words in text\n",
    "    - keep track of the count / frequency of each word\n",
    "- implement bag of word\n",
    "    - documents is a list holding the documents as strings in a list\n",
    "    - build a vocabulary:\n",
    "        - tokenize the documents and store all tokens in all_words\n",
    "        - remove stopwords\n",
    "        - remove punctuation\n",
    "        - ignore case\n",
    "        - do stemming or other lemmatization to ignore tense/plural\n",
    "        - remove duplicate words\n",
    "    - how to score the presence of known words\n",
    "        - generate vectors by counting instences of each word\n",
    "        - generate vectors using 0s and 1s representing a word was present or not\n",
    "        - frequency = count_word / count_total\n",
    "- limitations\n",
    "    - too sparse\n",
    "    - context loss since discarding word order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['also', 'football', 'games', 'john', 'likes', 'mary', 'movies', 'watch']\n",
      "John likes to watch movies. Mary likes Movies too. \n",
      "[0. 0. 0. 1. 2. 1. 2. 1.]\n",
      "\n",
      "Mary also likes to watch football games. \n",
      "[1. 1. 1. 0. 1. 1. 0. 1.]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "import string\n",
    "\n",
    "documents = [\"John likes to watch movies. Mary likes Movies too.\",\n",
    "             \"Mary also likes to watch football games.\"]\n",
    "\n",
    "## Create Tokens from the set of documents\n",
    "all_words = []\n",
    "for document in documents:\n",
    "    all_words.extend(word_tokenize(document))\n",
    "\n",
    "## ignore case and remove duplicates so only UNIQUE words remain\n",
    "all_words = [word.lower() for word in all_words] ## ADDED\n",
    "unique_words = sorted(list(set(all_words)))\n",
    "\n",
    "## Process out stop words and punctuation to create VOCABULARY\n",
    "stop_words = set(stopwords.words('english'))\n",
    "unique_words = filter(lambda word: word not in string.punctuation, unique_words)\n",
    "vocabulary = [element for element in unique_words if element.casefold() not in stop_words]\n",
    "\n",
    "print(vocabulary)\n",
    "\n",
    "\n",
    "## Generate bag of word vectors by counting instances of each word\n",
    "for document in documents:\n",
    "        words = word_tokenize(document)\n",
    "        bag_vector = np.zeros(len(vocabulary))\n",
    "        for w in words:\n",
    "            for i,word in enumerate(vocabulary):\n",
    "                if word.lower() == w.lower():\n",
    "                    bag_vector[i] += 1\n",
    "                    \n",
    "        print(\"{0} \\n{1}\\n\".format(document,np.array(bag_vector)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   also  football  games  john  likes  mary  movies  to  too  watch\n",
      "0     0         0      0     1      2     1       2   1    1      1\n",
      "1     1         1      1     0      1     1       0   1    0      1\n"
     ]
    }
   ],
   "source": [
    "# use Scikit-Learn CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "documents = [\"John likes to watch movies. Mary likes Movies too.\",\n",
    "             \"Mary also likes to watch football games.\"]\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(documents)\n",
    "\n",
    "df_bow_sklearn = pd.DataFrame(X.toarray(),columns=vectorizer.get_feature_names_out())\n",
    "print(df_bow_sklearn.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1 2 1 2 1]\n",
      " [1 1 0 1 1 0 1]]\n"
     ]
    }
   ],
   "source": [
    "# if remove stopwords\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "X = vectorizer.fit_transform(documents)\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if use binary score\n",
    "vectorizer = CountVectorizer(binary=True)\n",
    "X = vectorizer.fit_transform(documents)\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n-gram\n",
    "vectorizer = CountVectorizer(analyzer='word', ngram_range=(2, 2))\n",
    "X = vectorizer.fit_transform(documents)\n",
    "\n",
    "print(vectorizer.get_feature_names_out())\n",
    "print(X.toarray())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Term Frequency-Inverse Decument Frequency (TF-IDF)\n",
    "- sometimes refered as \"weighted\" bag of word\n",
    "- penalize words are frequent in all documents, i.e. \"the\"\n",
    "- TF-IDF scores have the effect of highlighting words that are distinct (contain useful information) in a given document.algorithm:\n",
    "    - TF(word, document) = count_word_in_document / total_word_in_document\n",
    "    - IDF(word) = log(count_documents / count_documents_contain_word)\n",
    "    - TF-IDF(word, document) = TF(word, document) * IDF(word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "import string\n",
    " \n",
    "documents = ['The dog plays fetch', 'The cat hunts bugs']\n",
    " \n",
    "# Create Tokens from the set of documents\n",
    "all_words = []\n",
    "for document in documents:\n",
    "    all_words.extend(word_tokenize(document))\n",
    "\n",
    "## remove duplicates so only UNIQUE words remain\n",
    "all_words_lower_case = [word.lower() for word in all_words]\n",
    "unique_words = sorted(list(set(all_words_lower_case)))\n",
    "\n",
    "## Process out stop words to create VOCABULARY\n",
    "stop_words = set(stopwords.words('english'))\n",
    "unique_words = filter(lambda word: word not in string.punctuation, unique_words)\n",
    "vocabulary = [element for element in unique_words if element.casefold() not in stop_words]\n",
    "\n",
    "total_documents = len(documents)\n",
    "total_documents\n",
    "\n",
    "vectors = []\n",
    "\n",
    "for document in documents:\n",
    "    doc_vec = np.zeros((len(vocabulary),)) # create vector of all 0's\n",
    "    index = 0 # tracks which vocabulary word we are on as vector index\n",
    "    for word in vocabulary:\n",
    "        \n",
    "        # calculate TF\n",
    "        occurences = len([token for token in document.split() if token.lower() == word])\n",
    "        words_in_doc = len(document.split())\n",
    "        tf = float(occurences/words_in_doc)\n",
    "\n",
    "        # calculate IDF\n",
    "        occurences = 0\n",
    "        for inner_document in documents:\n",
    "            if word in inner_document.lower():\n",
    "                occurences += 1\n",
    "\n",
    "        idf = np.log10(total_documents/occurences)\n",
    "\n",
    "        # store IDF into the vector\n",
    "        doc_vec[index] = tf * idf\n",
    "        index += 1\n",
    "\n",
    "    # inside outer loop but not inner loop\n",
    "    vectors.append(doc_vec) \n",
    "\n",
    "# outside both loops\n",
    "print(\"{0} \\n{1}\\n\".format(vocabulary,np.array(vectors)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use Scikit-Learn TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "documents = ['The dog plays with the ball', 'The cat plays with the ball']\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(documents)\n",
    "\n",
    "print(vectorizer.get_feature_names_out()) # prints vocabulary\n",
    "print(X.toarray()) # prints TF-IDF scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stopwords\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "X = vectorizer.fit_transform(documents)\n",
    "\n",
    "print(vectorizer.get_feature_names_out()) # prints vocabulary\n",
    "print(X.toarray()) # prints TF-IDF scores. it's different than previous cell since sklearn uses some improved algorithm, i.e. idf smooth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn off smooth_idf\n",
    "vectorizer = TfidfVectorizer(smooth_idf=False)\n",
    "\n",
    "X = vectorizer.fit_transform(documents)\n",
    "\n",
    "print(vectorizer.get_feature_names_out()) # prints vocabulary\n",
    "print(X.toarray()) # prints TF-IDF scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n-gram\n",
    "vectorizer = TfidfVectorizer(stop_words='english', analyzer='word', ngram_range=(2, 2))\n",
    "X = vectorizer.fit_transform(documents)\n",
    "\n",
    "print(vectorizer.get_feature_names_out())\n",
    "print(X.toarray()) # prints TF-IDF scores"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Text similarity metrics\n",
    "- Jaccard Similarity Coefficient Scorey: J(A,B) = (A and B) / (A or B)\n",
    "- Euclidean distance: d(p,q)^2 = (q1 - p1)^2 + (q2 - p2)^2 + ... + (qn - pn)^2\n",
    "- Cosine similarity: cos(theta) = (A.B) / x*y = (a1b1 + ... + anbn) / [(a1^2 + ... + an^2)^0.5 * (b1^2 + ... + bn^2)^0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jaccard Similarity\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import jaccard_score\n",
    "\n",
    "documents = [\"Bunnies like to eat lettuce more than carrots.\", \n",
    "             \"Fish like to play with bubbles while swimming.\"]\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words='english', binary=True)\n",
    "X = vectorizer.fit_transform(documents)\n",
    "\n",
    "print(jaccard_score(X.toarray()[0], X.toarray()[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Euclidean distance\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "documents = [\"Bunnies like to eat lettuce more than carrots.\", \n",
    "             \"Fish like to play with bubbles while swimming.\"]\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words='english', binary=True)\n",
    "X = vectorizer.fit_transform(documents)\n",
    "\n",
    "print( euclidean_distances(X) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cosine similarity\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "documents = [\"Bunnies like to eat lettuce more than carrots.\", \n",
    "             \"Fish like to play with bubbles while swimming.\"]\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words='english', binary=True)\n",
    "X = vectorizer.fit_transform(documents)\n",
    "\n",
    "print( cosine_similarity(X) )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 Language Models\n",
    "- A language model is a model which understands language – more precisely how words occur together in natural language. A language model is used to predict what word comes next.\n",
    "- probabilistic language models and machine learning language models\n",
    "- use pre-trained language models:\n",
    "    - What task are you using it for?\n",
    "    - What are the technical requirements to use the model?\n",
    "- Popular Pre-Trained Models\n",
    "    - BERT from Google\n",
    "    - GPT-3 from OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Using DialoGPT\n",
    "import os\n",
    "import transformers # need to install this package\n",
    "\n",
    "model = transformers.pipeline(\"conversational\", model=\"microsoft/DialoGPT-small\")\n",
    "\n",
    "## conversation runner code\n",
    "user = input(\"Enter text or type 'bye' to end: \").strip().lower()\n",
    "while user != 'bye':\n",
    "\n",
    "    # generate and print response using model\n",
    "    response = str(model(transformers.Conversation(user), pad_token_id=50256))\n",
    "    print(response[response.find(\"bot >> \")+6:].strip())\n",
    "\n",
    "    # get next user input\n",
    "    user = input(\"Enter text or type 'bye' to end: \").strip().lower()\n",
    "\n",
    "# end of chat\n",
    "print(\"Good bye!\")\n",
    "\n",
    "response = str(model(transformers.Conversation(user), pad_token_id=50256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Using Blenderbot\n",
    "import os\n",
    "import transformers\n",
    "\n",
    "#initialize pretrained model\n",
    "model = transformers.pipeline(\"conversational\", model=\"facebook/blenderbot_small-90M\")\n",
    "\n",
    "## conversation runner code\n",
    "user = input(\"Enter text or type 'bye' to end: \").strip().lower()\n",
    "while user != 'bye':\n",
    "\n",
    "    # generate and print response\n",
    "    response = str(model(transformers.Conversation(user)))\n",
    "    print(response[response.find(\"bot >> \")+6:].strip())\n",
    "\n",
    "    # get next user input\n",
    "    user = input(\"Enter text or type 'bye' to end: \").strip().lower()\n",
    "\n",
    "# end of chat\n",
    "print(\"Good bye!\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Lab: create a chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a simple hard-coded chatbot\n",
    "import tkinter.scrolledtext as tks #creates a scrollable text window\n",
    "from datetime import datetime\n",
    "from tkinter import *\n",
    "\n",
    "\n",
    "# Generating response\n",
    "def get_bot_response(user_input):\n",
    "\n",
    "  bot_response = \"\"\n",
    "  if(user_input == \"hello\"):\n",
    "    bot_response = \"Hi!\"\n",
    "  elif(user_input == \"hi\" or user_input == \"hii\" or user_input == \"hiiii\"):\n",
    "    bot_response = \"Hello there! How are you?\"\n",
    "  elif(user_input == \"how are you\"):\n",
    "    bot_response = \"Oh, I'm great! How about you?\"\n",
    "  elif(user_input == \"fine\" or user_input == \"i am good\" or user_input == \"i am doing good\"):\n",
    "    bot_response = \"That's excellent! How can I help you today?\"\n",
    "  else:\n",
    "    bot_response = \"I'm sorry, I don't understand...\"      \n",
    "    \n",
    "  return bot_response\n",
    "\n",
    "\n",
    "def create_and_insert_user_frame(user_input):\n",
    "  userFrame = Frame(chatWindow, bg=\"#d0ffff\")\n",
    "  Label(\n",
    "      userFrame,\n",
    "      text=user_input,\n",
    "      font=(\"Arial\", 11),\n",
    "      bg=\"#d0ffff\").grid(row=0, column=0, sticky=\"w\", padx=5, pady=5)\n",
    "  Label(\n",
    "      userFrame,\n",
    "      text=datetime.now().strftime(\"%H:%M\"),\n",
    "      font=(\"Arial\", 7),\n",
    "      bg=\"#d0ffff\"\n",
    "  ).grid(row=1, column=0, sticky=\"w\")\n",
    "\n",
    "  chatWindow.insert('end', '\\n ', 'tag-right')\n",
    "  chatWindow.window_create('end', window=userFrame)\n",
    "\n",
    "\n",
    "def create_and_insert_bot_frame(bot_response):\n",
    "  botFrame = Frame(chatWindow, bg=\"#ffffd0\")\n",
    "  Label(\n",
    "      botFrame,\n",
    "      text=bot_response,\n",
    "      font=(\"Arial\", 11),\n",
    "      bg=\"#ffffd0\",\n",
    "      wraplength=400,\n",
    "      justify='left'\n",
    "  ).grid(row=0, column=0, sticky=\"w\", padx=5, pady=5)\n",
    "  Label(\n",
    "      botFrame,\n",
    "      text=datetime.now().strftime(\"%H:%M\"),\n",
    "      font=(\"Arial\", 7),\n",
    "      bg=\"#ffffd0\"\n",
    "  ).grid(row=1, column=0, sticky=\"w\")\n",
    "\n",
    "  chatWindow.insert('end', '\\n ', 'tag-left')\n",
    "  chatWindow.window_create('end', window=botFrame)\n",
    "  chatWindow.insert(END, \"\\n\\n\" + \"\")\n",
    "\n",
    "\n",
    "def send(event):\n",
    "    chatWindow.config(state=NORMAL)\n",
    "\n",
    "    user_input = userEntryBox.get(\"1.0\",'end-2c')\n",
    "    user_input_lc = user_input.lower()\n",
    "    bot_response = get_bot_response(user_input_lc) \n",
    "\n",
    "    create_and_insert_user_frame(user_input)\n",
    "    create_and_insert_bot_frame(bot_response)\n",
    "\n",
    "    chatWindow.config(state=DISABLED)\n",
    "    userEntryBox.delete(\"1.0\",\"end\")\n",
    "    chatWindow.see('end')\n",
    "\n",
    "\n",
    "baseWindow = Tk()\n",
    "baseWindow.title(\"The Simple Bot\")\n",
    "baseWindow.geometry(\"500x250\")\n",
    "\n",
    "chatWindow = tks.ScrolledText(baseWindow, font=\"Arial\")\n",
    "chatWindow.tag_configure('tag-left', justify='left')\n",
    "chatWindow.tag_configure('tag-right', justify='right')\n",
    "chatWindow.config(state=DISABLED)\n",
    "\n",
    "sendButton = Button(\n",
    "    baseWindow,\n",
    "    font=(\"Verdana\", 12, 'bold'),\n",
    "    text=\"Send\",\n",
    "    bg=\"#fd94b4\",\n",
    "    activebackground=\"#ff467e\",\n",
    "    fg='#ffffff',\n",
    "    command=send)\n",
    "sendButton.bind(\"<Button-1>\", send)\n",
    "baseWindow.bind('<Return>', send)\n",
    "\n",
    "userEntryBox = Text(baseWindow, bd=1, bg=\"white\", width=38, font=\"Arial\")\n",
    "\n",
    "chatWindow.place(x=1, y=1, height=200, width=500)\n",
    "userEntryBox.place(x=3, y=202, height=27)\n",
    "sendButton.place(x=430, y=200)\n",
    "\n",
    "baseWindow.mainloop()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create chatbot with nltk.chat\n",
    "import tkinter.scrolledtext as tks #creates a scrollable text window\n",
    "\n",
    "from datetime import datetime\n",
    "from tkinter import *\n",
    "from nltk.chat.util import Chat\n",
    "\n",
    "\n",
    "# Generating response\n",
    "def get_bot_response(user_input):\n",
    "  pairs = [\n",
    "    ('my name is (.*)', ['Hello ! % 1']),\n",
    "    ('(hi|hello|hey|holla|hola)', ['Hey there !', 'Hi there !', 'Hey !']),\n",
    "    ('(.*) your name ?', ['My name is Geeky']),\n",
    "    ('(.*) do you do ?', ['We provide a platform for tech enthusiasts, a wide range of options !']),\n",
    "    ('(.*) created you ?', ['Geeksforgeeks created me using python and NLTK'])\n",
    "  ]\n",
    "\n",
    "  chat = Chat(pairs)  \n",
    "  while user_input[-1] in \"!.\":\n",
    "    user_input = user_input[:-1]\n",
    "  bot_response = chat.respond(user_input)   \n",
    "  return bot_response\n",
    "\n",
    "\n",
    "def create_and_insert_user_frame(user_input):\n",
    "  userFrame = Frame(chatWindow, bg=\"#d0ffff\")\n",
    "  Label(\n",
    "      userFrame,\n",
    "      text=user_input,\n",
    "      font=(\"Arial\", 11),\n",
    "      bg=\"#d0ffff\").grid(row=0, column=0, sticky=\"w\", padx=5, pady=5)\n",
    "  Label(\n",
    "      userFrame,\n",
    "      text=datetime.now().strftime(\"%H:%M\"),\n",
    "      font=(\"Arial\", 7),\n",
    "      bg=\"#d0ffff\"\n",
    "  ).grid(row=1, column=0, sticky=\"w\")\n",
    "\n",
    "  chatWindow.insert('end', '\\n ', 'tag-right')\n",
    "  chatWindow.window_create('end', window=userFrame)\n",
    "\n",
    "\n",
    "def create_and_insert_bot_frame(bot_response):\n",
    "  botFrame = Frame(chatWindow, bg=\"#ffffd0\")\n",
    "  Label(\n",
    "      botFrame,\n",
    "      text=bot_response,\n",
    "      font=(\"Arial\", 11),\n",
    "      bg=\"#ffffd0\",\n",
    "      wraplength=400,\n",
    "      justify='left'\n",
    "  ).grid(row=0, column=0, sticky=\"w\", padx=5, pady=5)\n",
    "  Label(\n",
    "      botFrame,\n",
    "      text=datetime.now().strftime(\"%H:%M\"),\n",
    "      font=(\"Arial\", 7),\n",
    "      bg=\"#ffffd0\"\n",
    "  ).grid(row=1, column=0, sticky=\"w\")\n",
    "\n",
    "  chatWindow.insert('end', '\\n ', 'tag-left')\n",
    "  chatWindow.window_create('end', window=botFrame)\n",
    "  chatWindow.insert(END, \"\\n\\n\" + \"\")\n",
    "\n",
    "\n",
    "def send(event):\n",
    "    chatWindow.config(state=NORMAL)\n",
    "\n",
    "    user_input = userEntryBox.get(\"1.0\",'end-2c')\n",
    "    user_input_lc = user_input.lower()\n",
    "    bot_response = get_bot_response(user_input_lc) \n",
    "\n",
    "    create_and_insert_user_frame(user_input)\n",
    "    create_and_insert_bot_frame(bot_response)\n",
    "\n",
    "    chatWindow.config(state=DISABLED)\n",
    "    userEntryBox.delete(\"1.0\",\"end\")\n",
    "    chatWindow.see('end')\n",
    "\n",
    "\n",
    "baseWindow = Tk()\n",
    "baseWindow.title(\"The Simple Bot\")\n",
    "baseWindow.geometry(\"500x250\")\n",
    "\n",
    "chatWindow = tks.ScrolledText(baseWindow, font=\"Arial\")\n",
    "chatWindow.tag_configure('tag-left', justify='left')\n",
    "chatWindow.tag_configure('tag-right', justify='right')\n",
    "chatWindow.config(state=DISABLED)\n",
    "\n",
    "sendButton = Button(\n",
    "    baseWindow,\n",
    "    font=(\"Verdana\", 12, 'bold'),\n",
    "    text=\"Send\",\n",
    "    bg=\"#fd94b4\",\n",
    "    activebackground=\"#ff467e\",\n",
    "    fg='#ffffff',\n",
    "    command=send)\n",
    "sendButton.bind(\"<Button-1>\", send)\n",
    "baseWindow.bind('<Return>', send)\n",
    "\n",
    "userEntryBox = Text(baseWindow, bd=1, bg=\"white\", width=38, font=\"Arial\")\n",
    "\n",
    "chatWindow.place(x=1, y=1, height=200, width=500)\n",
    "userEntryBox.place(x=3, y=202, height=27)\n",
    "sendButton.place(x=430, y=200)\n",
    "\n",
    "baseWindow.mainloop()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create chatbot with google search\n",
    "import string\n",
    "import nltk\n",
    "import requests\n",
    "\n",
    "from googlesearch import search\n",
    "from lxml import html\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "# helper function to generate text corpus from html elements\n",
    "def generate_corpus(all_p_elements):\n",
    "    corpus = \"\"\n",
    "    for p_element in all_p_elements:\n",
    "        corpus += '\\n' + ''.join(p_element.findAll(text = True))\n",
    "    return corpus\n",
    "\n",
    "\n",
    "# preprocessing (lemmatization, removing punctuation)\n",
    "def LemTokens(tokens):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "\n",
    "def LemNormalize(text):\n",
    "    remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
    "    return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))\n",
    "\n",
    "\n",
    "def calculate_cosine_similarity(tfidf):\n",
    "\n",
    "    # Calculate cosine_similarity\n",
    "    cosine_sim_matrix = cosine_similarity(tfidf[-1], tfidf)\n",
    "    \n",
    "    # get index second highest value for cosine_similarity \n",
    "    # highest value will be the user_input itself\n",
    "    idx = cosine_sim_matrix.argsort()[0][-2]\n",
    "    cosine_sim_flattened = cosine_sim_matrix.flatten()\n",
    "    cosine_sim_flattened.sort()\n",
    "    req_tfidf = cosine_sim_flattened[-2]\n",
    "\n",
    "    return idx, req_tfidf\n",
    "\n",
    "\n",
    "def get_bot_response(user_input):\n",
    "    # default bot response\n",
    "    bot_response = \"I'm sorry, I don't think I can help you with that :(\"\n",
    "    try:\n",
    "        # use the google search api to fetch top 3 search results\n",
    "        google_search_results = list(search(user_input, stop=3, pause=1))\n",
    "        \n",
    "        # use the requests api to fetch the top result webpage\n",
    "        webpage = requests.get(google_search_results[0])\n",
    "        webpage_tree = html.fromstring(webpage.content)\n",
    "        webpage_soup = BeautifulSoup(webpage.content, \"lxml\")\n",
    "        \n",
    "        # extract all <p> elements from webpage soup object\n",
    "        all_p_list = webpage_soup.findAll('p')\n",
    "        \n",
    "        # generate corpus from all <p> elements\n",
    "        google_search_corpus = generate_corpus(all_p_list)\n",
    "\n",
    "        # Tokenisation\n",
    "        sentence_tokens = nltk.sent_tokenize(google_search_corpus)# converts raw text to list of sentences\n",
    "\n",
    "        # Calculate TFIDF matrix\n",
    "        sentence_tokens.append(user_input)\n",
    "        tfidf_vectorizer = TfidfVectorizer(tokenizer=LemNormalize, stop_words='english')\n",
    "        tfidf = tfidf_vectorizer.fit_transform(sentence_tokens)\n",
    "\n",
    "        idx, req_tfidf = calculate_cosine_similarity(tfidf)\n",
    "        \n",
    "        if(req_tfidf==0):\n",
    "            # if value of cosine_similarity == 0, similar sentence not found \n",
    "            bot_response = \"I am sorry! I don't think I can help you with that at the moment...\"\n",
    "        else:\n",
    "            bot_response = sentence_tokens[idx]\n",
    "        sentence_tokens.remove(user_input)\n",
    "        return bot_response\n",
    "\n",
    "    except:\n",
    "      # return the default response if corpus is empty\n",
    "      if len(google_search_corpus) == 0:\n",
    "        return bot_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate chatbot with a pre-trained model\n",
    "import os\n",
    "import transformers\n",
    "\n",
    "\n",
    "def initialize_model():\n",
    "\n",
    "  model = transformers.pipeline(\"conversational\", model=\"facebook/blenderbot_small-90M\")\n",
    "  os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "\n",
    "  return model\n",
    "\n",
    "\n",
    "def get_bot_response(model, user_input):\n",
    "\n",
    "  chat = model(transformers.Conversation(user_input))\n",
    "  bot_response = str(chat)\n",
    "  bot_response = bot_response[bot_response.find(\"bot >> \")+6:].strip()\n",
    "\n",
    "  return bot_response"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
